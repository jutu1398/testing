{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from head import ClsCntRegHead\n",
    "from fpn import FPN\n",
    "from backbone.Shufflenet import shufflenet_v2_x1_0\n",
    "import torch.nn as nn\n",
    "from loss import GenTargets,LOSS,coords_fmap2orig\n",
    "import torch\n",
    "from config import DefaultConfig\n",
    "\n",
    "\n",
    "\n",
    "class FCOS(nn.Module):\n",
    "    \n",
    "    def __init__(self,config=None):\n",
    "        super().__init__()\n",
    "        if config is None:\n",
    "            config=DefaultConfig\n",
    "        self.backbone=shufflenet_v2_x1_0(pretrained=config.pretrained)\n",
    "        self.fpn=FPN(config.fpn_out_channels,use_p5=config.use_p5)\n",
    "        self.head=ClsCntRegHead(config.fpn_out_channels,config.class_num,\n",
    "                                config.use_GN_head,config.cnt_on_reg,config.prior)\n",
    "        self.config=config\n",
    "    def train(self,mode=True):\n",
    "        '''\n",
    "        set module training mode, and frozen bn\n",
    "        '''\n",
    "        super().train(mode=True)\n",
    "        def freeze_bn(module):\n",
    "            if isinstance(module,nn.BatchNorm2d):\n",
    "                module.eval()\n",
    "            classname = module.__class__.__name__\n",
    "            if classname.find('BatchNorm') != -1:\n",
    "                for p in module.parameters(): p.requires_grad=False\n",
    "        if self.config.freeze_bn:\n",
    "            self.apply(freeze_bn)\n",
    "            print(\"INFO===>success frozen BN\")\n",
    "        if self.config.freeze_stage_1:\n",
    "            self.backbone.freeze_stages(1)\n",
    "            print(\"INFO===>success frozen backbone stage1\")\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Returns\n",
    "        list [cls_logits,cnt_logits,reg_preds]  \n",
    "        cls_logits  list contains five [batch_size,class_num,h,w]\n",
    "        cnt_logits  list contains five [batch_size,1,h,w]\n",
    "        reg_preds   list contains five [batch_size,4,h,w]\n",
    "        '''\n",
    "        C3,C4,C5=self.backbone(x)\n",
    "        all_P=self.fpn([C3,C4,C5])\n",
    "        cls_logits,cnt_logits,reg_preds=self.head(all_P)\n",
    "        return [cls_logits,cnt_logits,reg_preds]\n",
    "\n",
    "class DetectHead(nn.Module):\n",
    "    def __init__(self,score_threshold,nms_iou_threshold,max_detection_boxes_num,strides,config=None):\n",
    "        super().__init__()\n",
    "        self.score_threshold=score_threshold\n",
    "        self.nms_iou_threshold=nms_iou_threshold\n",
    "        self.max_detection_boxes_num=max_detection_boxes_num\n",
    "        self.strides=strides\n",
    "        if config is None:\n",
    "            self.config=DefaultConfig\n",
    "        else:\n",
    "            self.config=config\n",
    "    def forward(self,inputs):\n",
    "        '''\n",
    "        inputs  list [cls_logits,cnt_logits,reg_preds]  \n",
    "        cls_logits  list contains five [batch_size,class_num,h,w]  \n",
    "        cnt_logits  list contains five [batch_size,1,h,w]  \n",
    "        reg_preds   list contains five [batch_size,4,h,w] \n",
    "        '''\n",
    "        cls_logits,coords=self._reshape_cat_out(inputs[0],self.strides)#[batch_size,sum(_h*_w),class_num]\n",
    "        cnt_logits,_=self._reshape_cat_out(inputs[1],self.strides)#[batch_size,sum(_h*_w),1]\n",
    "        reg_preds,_=self._reshape_cat_out(inputs[2],self.strides)#[batch_size,sum(_h*_w),4]\n",
    "\n",
    "        cls_preds=cls_logits.sigmoid_()\n",
    "        cnt_preds=cnt_logits.sigmoid_()\n",
    "\n",
    "        cls_scores,cls_classes=torch.max(cls_preds,dim=-1)#[batch_size,sum(_h*_w)]\n",
    "        if self.config.add_centerness:\n",
    "            cls_scores=cls_scores*(cnt_preds.squeeze(dim=-1))#[batch_size,sum(_h*_w)]\n",
    "        cls_classes=cls_classes+1#[batch_size,sum(_h*_w)]\n",
    "\n",
    "        boxes=self._coords2boxes(coords,reg_preds)#[batch_size,sum(_h*_w),4]\n",
    "\n",
    "        #select topk\n",
    "        max_num=min(self.max_detection_boxes_num,cls_scores.shape[-1])\n",
    "        topk_ind=torch.topk(cls_scores,max_num,dim=-1,largest=True,sorted=True)[1]#[batch_size,max_num]\n",
    "        _cls_scores=[]\n",
    "        _cls_classes=[]\n",
    "        _boxes=[]\n",
    "        for batch in range(cls_scores.shape[0]):\n",
    "            _cls_scores.append(cls_scores[batch][topk_ind[batch]])#[max_num]\n",
    "            _cls_classes.append(cls_classes[batch][topk_ind[batch]])#[max_num]\n",
    "            _boxes.append(boxes[batch][topk_ind[batch]])#[max_num,4]\n",
    "        cls_scores_topk=torch.stack(_cls_scores,dim=0)#[batch_size,max_num]\n",
    "        cls_classes_topk=torch.stack(_cls_classes,dim=0)#[batch_size,max_num]\n",
    "        boxes_topk=torch.stack(_boxes,dim=0)#[batch_size,max_num,4]\n",
    "        assert boxes_topk.shape[-1]==4\n",
    "        return self._post_process([cls_scores_topk,cls_classes_topk,boxes_topk])\n",
    "\n",
    "    def _post_process(self,preds_topk):\n",
    "        '''\n",
    "        cls_scores_topk [batch_size,max_num]\n",
    "        cls_classes_topk [batch_size,max_num]\n",
    "        boxes_topk [batch_size,max_num,4]\n",
    "        '''\n",
    "        _cls_scores_post=[]\n",
    "        _cls_classes_post=[]\n",
    "        _boxes_post=[]\n",
    "        cls_scores_topk,cls_classes_topk,boxes_topk=preds_topk\n",
    "        for batch in range(cls_classes_topk.shape[0]):\n",
    "            mask=cls_scores_topk[batch]>=self.score_threshold\n",
    "            _cls_scores_b=cls_scores_topk[batch][mask]#[?]\n",
    "            _cls_classes_b=cls_classes_topk[batch][mask]#[?]\n",
    "            _boxes_b=boxes_topk[batch][mask]#[?,4]\n",
    "            nms_ind=self.batched_nms(_boxes_b,_cls_scores_b,_cls_classes_b,self.nms_iou_threshold)\n",
    "            _cls_scores_post.append(_cls_scores_b[nms_ind])\n",
    "            _cls_classes_post.append(_cls_classes_b[nms_ind])\n",
    "            _boxes_post.append(_boxes_b[nms_ind])\n",
    "        scores,classes,boxes= torch.stack(_cls_scores_post,dim=0),torch.stack(_cls_classes_post,dim=0),torch.stack(_boxes_post,dim=0)\n",
    "        \n",
    "        return scores,classes,boxes\n",
    "    \n",
    "    @staticmethod\n",
    "    def box_nms(boxes,scores,thr):\n",
    "        '''\n",
    "        boxes: [?,4]\n",
    "        scores: [?]\n",
    "        '''\n",
    "        if boxes.shape[0]==0:\n",
    "            return torch.zeros(0,device=boxes.device).long()\n",
    "        assert boxes.shape[-1]==4\n",
    "        x1,y1,x2,y2=boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]\n",
    "        areas=(x2-x1+1)*(y2-y1+1)\n",
    "        order=scores.sort(0,descending=True)[1]\n",
    "        keep=[]\n",
    "        while order.numel()>0:\n",
    "            if order.numel()==1:\n",
    "                i=order.item()\n",
    "                keep.append(i)\n",
    "                break\n",
    "            else:\n",
    "                i=order[0].item()\n",
    "                keep.append(i)\n",
    "            \n",
    "            xmin=x1[order[1:]].clamp(min=float(x1[i]))\n",
    "            ymin=y1[order[1:]].clamp(min=float(y1[i]))\n",
    "            xmax=x2[order[1:]].clamp(max=float(x2[i]))\n",
    "            ymax=y2[order[1:]].clamp(max=float(y2[i]))\n",
    "            inter=(xmax-xmin).clamp(min=0)*(ymax-ymin).clamp(min=0)\n",
    "            iou=inter/(areas[i]+areas[order[1:]]-inter)\n",
    "            idx=(iou<=thr).nonzero().squeeze()\n",
    "            if idx.numel()==0:\n",
    "                break\n",
    "            order=order[idx+1]\n",
    "        return torch.LongTensor(keep)\n",
    "\n",
    "    def batched_nms(self,boxes, scores, idxs, iou_threshold):\n",
    "        \n",
    "        if boxes.numel() == 0:\n",
    "            return torch.empty((0,), dtype=torch.int64, device=boxes.device)\n",
    "        # strategy: in order to perform NMS independently per class.\n",
    "        # we add an offset to all the boxes. The offset is dependent\n",
    "        # only on the class idx, and is large enough so that boxes\n",
    "        # from different classes do not overlap\n",
    "        max_coordinate = boxes.max()\n",
    "        offsets = idxs.to(boxes) * (max_coordinate + 1)\n",
    "        boxes_for_nms = boxes + offsets[:, None]\n",
    "        keep = self.box_nms(boxes_for_nms, scores, iou_threshold)\n",
    "        return keep\n",
    "\n",
    "    def _coords2boxes(self,coords,offsets):\n",
    "        '''\n",
    "        Args\n",
    "        coords [sum(_h*_w),2]\n",
    "        offsets [batch_size,sum(_h*_w),4] ltrb\n",
    "        '''\n",
    "        x1y1=coords[None,:,:]-offsets[...,:2]\n",
    "        x2y2=coords[None,:,:]+offsets[...,2:]#[batch_size,sum(_h*_w),2]\n",
    "        boxes=torch.cat([x1y1,x2y2],dim=-1)#[batch_size,sum(_h*_w),4]\n",
    "        return boxes\n",
    "\n",
    "\n",
    "    def _reshape_cat_out(self,inputs,strides):\n",
    "        '''\n",
    "        Args\n",
    "        inputs: list contains five [batch_size,c,_h,_w]\n",
    "        Returns\n",
    "        out [batch_size,sum(_h*_w),c]\n",
    "        coords [sum(_h*_w),2]\n",
    "        '''\n",
    "        batch_size=inputs[0].shape[0]\n",
    "        c=inputs[0].shape[1]\n",
    "        out=[]\n",
    "        coords=[]\n",
    "        for pred,stride in zip(inputs,strides):\n",
    "            pred=pred.permute(0,2,3,1)\n",
    "            coord=coords_fmap2orig(pred,stride).to(device=pred.device)\n",
    "            pred=torch.reshape(pred,[batch_size,-1,c])\n",
    "            out.append(pred)\n",
    "            coords.append(coord)\n",
    "        return torch.cat(out,dim=1),torch.cat(coords,dim=0)\n",
    "\n",
    "class ClipBoxes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,batch_imgs,batch_boxes):\n",
    "        batch_boxes=batch_boxes.clamp_(min=0)\n",
    "        h,w=batch_imgs.shape[2:]\n",
    "        batch_boxes[...,[0,2]]=batch_boxes[...,[0,2]].clamp_(max=w-1)\n",
    "        batch_boxes[...,[1,3]]=batch_boxes[...,[1,3]].clamp_(max=h-1)\n",
    "        return batch_boxes\n",
    "\n",
    "        \n",
    "class FCOSDetector(nn.Module):\n",
    "    def __init__(self,mode=\"training\",config=None):\n",
    "        super().__init__()\n",
    "        if config is None:\n",
    "            config=DefaultConfig\n",
    "        self.mode=mode\n",
    "        self.fcos_body=FCOS(config=config)\n",
    "        if mode==\"training\":\n",
    "            self.target_layer=GenTargets(strides=config.strides,limit_range=config.limit_range)\n",
    "            self.loss_layer=LOSS()\n",
    "        elif mode==\"inference\":\n",
    "            self.detection_head=DetectHead(config.score_threshold,config.nms_iou_threshold,\n",
    "                                            config.max_detection_boxes_num,config.strides,config)\n",
    "            self.clip_boxes=ClipBoxes()\n",
    "        \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        '''\n",
    "        inputs \n",
    "        [training] list  batch_imgs,batch_boxes,batch_classes\n",
    "        [inference] img\n",
    "        '''\n",
    "\n",
    "        if self.mode==\"training\":\n",
    "            batch_imgs,batch_boxes,batch_classes=inputs\n",
    "            out=self.fcos_body(batch_imgs)\n",
    "            targets=self.target_layer([out,batch_boxes,batch_classes])\n",
    "            losses=self.loss_layer([out,targets])\n",
    "            return losses\n",
    "        elif self.mode==\"inference\":\n",
    "            # raise NotImplementedError(\"no implement inference model\")\n",
    "            '''\n",
    "            for inference mode, img should preprocessed before feeding in net \n",
    "            '''\n",
    "            batch_imgs=inputs\n",
    "            out=self.fcos_body(batch_imgs)\n",
    "            scores,classes,boxes=self.detection_head(out)\n",
    "            boxes=self.clip_boxes(batch_imgs,boxes)\n",
    "            return scores,classes,boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model.fcos import FCOSDetector\n",
    "import torch\n",
    "from dataloader.VOC_dataset import VOCDataset\n",
    "import math,time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_dataset=VOCDataset(r\"E:\\Datasets\\VOCtrainval_11-May-2012\\VOCdevkit\\VOC2012\",resize_size=[512,800],split='train')\n",
    "val_dataset=VOCDataset(r\"E:\\Datasets\\VOC2012test\\VOCdevkit\\VOC2012\",resize_size=[512,800],split='val2007')\n",
    "\n",
    "model=FCOSDetector(mode=\"training\").cuda()\n",
    "#model.load_state_dict(torch.load(r\"C:\\Users\\JT\\Desktop\\CU Boulder\\Fall 2020\\Deep Learning\\fcos_attempt6\\FCOS.Pytorch - Different Net\\model\\voc2012_512x800_epoch10_loss1.1592 - Copy.pth\"))\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "BATCH_SIZE=6\n",
    "EPOCHS=20\n",
    "WARMPUP_STEPS_RATIO=0.12\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=train_dataset.collate_fn)\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=val_dataset.collate_fn)\n",
    "steps_per_epoch=len(train_dataset)//BATCH_SIZE\n",
    "TOTAL_STEPS=steps_per_epoch*EPOCHS\n",
    "WARMPUP_STEPS=TOTAL_STEPS*WARMPUP_STEPS_RATIO\n",
    "\n",
    "GLOBAL_STEPS=1\n",
    "LR_INIT=5e-5\n",
    "LR_END=1e-6\n",
    "\n",
    "writer=SummaryWriter(log_dir=\"./logs\")\n",
    "\n",
    "def lr_func():\n",
    "    if GLOBAL_STEPS<WARMPUP_STEPS:\n",
    "        lr=GLOBAL_STEPS/WARMPUP_STEPS*LR_INIT\n",
    "    else:\n",
    "        lr=LR_END+0.5*(LR_INIT-LR_END)*(\n",
    "            (1+math.cos((GLOBAL_STEPS-WARMPUP_STEPS)/(TOTAL_STEPS-WARMPUP_STEPS)*math.pi))\n",
    "        )\n",
    "    return float(lr)\n",
    "\n",
    "model.train()\n",
    "count=0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for epoch_step,data in enumerate(train_loader):\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        batch_imgs,batch_boxes,batch_classes=data\n",
    "        batch_imgs=batch_imgs.cuda()\n",
    "        batch_boxes=batch_boxes.cuda()\n",
    "        batch_classes=batch_classes.cuda()\n",
    "\n",
    "        lr=lr_func()\n",
    "        for param in optimizer.param_groups:\n",
    "            param['lr']=lr\n",
    "        \n",
    "        start_time=time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses=model([batch_imgs,batch_boxes,batch_classes])\n",
    "        loss=losses[-1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end_time=time.time()\n",
    "        cost_time=int((end_time-start_time)*1000)\n",
    "\n",
    "        print(\"global_steps:%d epoch:%d steps:%d/%d cls_loss:%.4f cnt_loss:%.4f reg_loss:%.4f cost_time:%dms lr=%.4e\"%\\\n",
    "            (GLOBAL_STEPS,epoch+1,epoch_step+1,steps_per_epoch,losses[0],losses[1],losses[2],cost_time,lr))\n",
    "       \n",
    "    \n",
    "        if count%500 == 0:\n",
    "            writer.add_scalar(\"loss/cls_loss\",losses[0],global_step=GLOBAL_STEPS)\n",
    "            writer.add_scalar(\"loss/cnt_loss\",losses[1],global_step=GLOBAL_STEPS)\n",
    "            writer.add_scalar(\"loss/reg_loss\",losses[2],global_step=GLOBAL_STEPS)\n",
    "            writer.add_scalar(\"lr\",lr,global_step=GLOBAL_STEPS)\n",
    "\n",
    "        GLOBAL_STEPS+=1\n",
    "    \n",
    "    torch.save(model.state_dict(),\"./voc2012_512x800_epoch%d_loss%.4f.pth\"%(epoch+11,loss.item()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO=====>voc dataset init finished  ! !\n",
      "INFO===>eval dataset has 5823 imgs\n",
      "INFO===>success frozen BN\n",
      "INFO===>success frozen backbone stage1\n",
      "===>success loading model\n",
      "3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-a79fb7aa4e60>:159: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:962.)\n",
      "  idx=(iou<=thr).nonzero().squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all classes AP=====>\n",
      " {1: 0.52579240099805, 2: 0.4871210418924896, 3: 0.40065430489446396, 4: 0.14510101464425718, 5: 0.16261634030491492, 6: 0.5383277017056077, 7: 0.2827181832336161, 8: 0.6474697117449095, 9: 0.15356679240616097, 10: 0.22904037138320837, 11: 0.21606616194542247, 12: 0.5641707025931217, 13: 0.48807339346743717, 14: 0.449838596773645, 15: 0.47690628650000544, 16: 0.08317510791739123, 17: 0.2525768619323994, 18: 0.24582848536955218, 19: 0.5359074622318072, 20: 0.22249627257762444, 21: 0.0}\n",
      "mAP=====>0.323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def sort_by_score(pred_boxes, pred_labels, pred_scores):\n",
    "    score_seq = [(-score).argsort() for index, score in enumerate(pred_scores)]\n",
    "    pred_boxes = [sample_boxes[mask] for sample_boxes, mask in zip(pred_boxes, score_seq)]\n",
    "    pred_labels = [sample_boxes[mask] for sample_boxes, mask in zip(pred_labels, score_seq)]\n",
    "    pred_scores = [sample_boxes[mask] for sample_boxes, mask in zip(pred_scores, score_seq)]\n",
    "    return pred_boxes, pred_labels, pred_scores\n",
    "\n",
    "def iou_2d(cubes_a, cubes_b):\n",
    "    \"\"\"\n",
    "    :param cubes_a: [N,(x1,y1,x2,y2)]\n",
    "    :param cubes_b: [M,(x1,y1,x2,y2)]\n",
    "    :return:  IoU [N,M]\n",
    "    \"\"\"\n",
    "\n",
    "    cubes_a = np.expand_dims(cubes_a, axis=1)  # [N,1,4]\n",
    "    cubes_b = np.expand_dims(cubes_b, axis=0)  # [1,M,4]\n",
    "\n",
    "    overlap = np.maximum(0.0,\n",
    "                         np.minimum(cubes_a[..., 2:], cubes_b[..., 2:]) -\n",
    "                         np.maximum(cubes_a[..., :2], cubes_b[..., :2]))  # [N,M,(w,h)]\n",
    "\n",
    "\n",
    "    overlap = np.prod(overlap, axis=-1)  # [N,M]\n",
    "\n",
    "    area_a = np.prod(cubes_a[..., 2:] - cubes_a[..., :2], axis=-1)\n",
    "    area_b = np.prod(cubes_b[..., 2:] - cubes_b[..., :2], axis=-1)\n",
    "\n",
    "    iou = overlap / (area_a + area_b - overlap)\n",
    "    return iou\n",
    "\n",
    "def _compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def eval_ap_2d(gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, iou_thread, num_cls):\n",
    "    \"\"\"\n",
    "    :param gt_boxes: list of 2d array,shape[(a,(x1,y1,x2,y2)),(b,(x1,y1,x2,y2))...]\n",
    "    :param gt_labels: list of 1d array,shape[(a),(b)...],value is sparse label index\n",
    "    :param pred_boxes: list of 2d array, shape[(m,(x1,y1,x2,y2)),(n,(x1,y1,x2,y2))...]\n",
    "    :param pred_labels: list of 1d array,shape[(m),(n)...],value is sparse label index\n",
    "    :param pred_scores: list of 1d array,shape[(m),(n)...]\n",
    "    :param iou_thread: eg. 0.5\n",
    "    :param num_cls: eg. 4, total number of class including background which is equal to 0\n",
    "    :return: a dict containing average precision for each cls\n",
    "    \"\"\"\n",
    "    all_ap = {}\n",
    "    for label in range(num_cls)[1:]:\n",
    "        # get samples with specific label\n",
    "        true_label_loc = [sample_labels == label for sample_labels in gt_labels]\n",
    "        gt_single_cls = [sample_boxes[mask] for sample_boxes, mask in zip(gt_boxes, true_label_loc)]\n",
    "\n",
    "        pred_label_loc = [sample_labels == label for sample_labels in pred_labels]\n",
    "        bbox_single_cls = [sample_boxes[mask] for sample_boxes, mask in zip(pred_boxes, pred_label_loc)]\n",
    "        scores_single_cls = [sample_scores[mask] for sample_scores, mask in zip(pred_scores, pred_label_loc)]\n",
    "\n",
    "        fp = np.zeros((0,))\n",
    "        tp = np.zeros((0,))\n",
    "        scores = np.zeros((0,))\n",
    "        total_gts = 0\n",
    "        # loop for each sample\n",
    "        for sample_gts, sample_pred_box, sample_scores in zip(gt_single_cls, bbox_single_cls, scores_single_cls):\n",
    "            total_gts = total_gts + len(sample_gts)\n",
    "            assigned_gt = []  # one gt can only be assigned to one predicted bbox\n",
    "            # loop for each predicted bbox\n",
    "            for index in range(len(sample_pred_box)):\n",
    "                scores = np.append(scores, sample_scores[index])\n",
    "                if len(sample_gts) == 0:  # if no gts found for the predicted bbox, assign the bbox to fp\n",
    "                    fp = np.append(fp, 1)\n",
    "                    tp = np.append(tp, 0)\n",
    "                    continue\n",
    "                pred_box = np.expand_dims(sample_pred_box[index], axis=0)\n",
    "                iou = iou_2d(sample_gts, pred_box)\n",
    "                gt_for_box = np.argmax(iou, axis=0)\n",
    "                max_overlap = iou[gt_for_box, 0]\n",
    "                if max_overlap >= iou_thread and gt_for_box not in assigned_gt:\n",
    "                    fp = np.append(fp, 0)\n",
    "                    tp = np.append(tp, 1)\n",
    "                    assigned_gt.append(gt_for_box)\n",
    "                else:\n",
    "                    fp = np.append(fp, 1)\n",
    "                    tp = np.append(tp, 0)\n",
    "        # sort by score\n",
    "        indices = np.argsort(-scores)\n",
    "        fp = fp[indices]\n",
    "        tp = tp[indices]\n",
    "        # compute cumulative false positives and true positives\n",
    "        fp = np.cumsum(fp)\n",
    "        tp = np.cumsum(tp)\n",
    "        # compute recall and precision\n",
    "        recall = tp / total_gts\n",
    "        precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        ap = _compute_ap(recall, precision)\n",
    "        all_ap[label] = ap\n",
    "        # print(recall, precision)\n",
    "    return all_ap\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #from model.fcos import FCOSDetector\n",
    "    from demo import convertSyncBNtoBN\n",
    "    from dataloader.VOC_dataset import VOCDataset\n",
    "    \n",
    "\n",
    "    eval_dataset=VOCDataset(r\"E:\\Datasets\\VOCtrainval_11-May-2012\\VOCdevkit\\VOC2012\",resize_size=[800,1024],split='val2007')\n",
    "    print(\"INFO===>eval dataset has %d imgs\"%len(eval_dataset))\n",
    "    eval_loader=torch.utils.data.DataLoader(eval_dataset,batch_size=1,shuffle=False,collate_fn=eval_dataset.collate_fn)\n",
    "\n",
    "    model=FCOSDetector(mode=\"inference\")\n",
    "    # model=torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    # print(\"INFO===>success convert BN to SyncBN\")\n",
    "    model.load_state_dict(torch.load(\"./Shufflenet_20epochs.pth\"))\n",
    "    # model=convertSyncBNtoBN(model)\n",
    "    # print(\"INFO===>success convert SyncBN to BN\")\n",
    "    model=model.cuda().eval()\n",
    "    print(\"===>success loading model\")\n",
    "\n",
    "    gt_boxes=[]\n",
    "    gt_classes=[]\n",
    "    pred_boxes=[]\n",
    "    pred_classes=[]\n",
    "    pred_scores=[]\n",
    "    num=0\n",
    "    for img,boxes,classes in eval_loader:\n",
    "        with torch.no_grad():\n",
    "            out=model(img.cuda())\n",
    "            pred_boxes.append(out[2][0].cpu().numpy())\n",
    "            pred_classes.append(out[1][0].cpu().numpy())\n",
    "            pred_scores.append(out[0][0].cpu().numpy())\n",
    "        gt_boxes.append(boxes[0].numpy())\n",
    "        gt_classes.append(classes[0].numpy())\n",
    "        num+=1\n",
    "        print(num,end='\\r')\n",
    "\n",
    "    # print(gt_boxes[0],gt_classes[0])\n",
    "    # print(pred_boxes[0],pred_classes[0],pred_scores[0])\n",
    "\n",
    "    pred_boxes,pred_classes,pred_scores=sort_by_score(pred_boxes,pred_classes,pred_scores)\n",
    "    all_AP=eval_ap_2d(gt_boxes,gt_classes,pred_boxes,pred_classes,pred_scores,0.5,len(eval_dataset.CLASSES_NAME)+1)\n",
    "    print(\"all classes AP=====>\\n\",all_AP)\n",
    "    mAP=0.\n",
    "    for class_id,class_mAP in all_AP.items():\n",
    "        mAP+=float(class_mAP)\n",
    "    mAP/=(len(eval_dataset.CLASSES_NAME)+1)\n",
    "    print(\"mAP=====>%.3f\\n\"%mAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
